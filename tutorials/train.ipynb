{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-06 07:23:40.211203: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-06 07:23:40.225172: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-06 07:23:40.229490: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-06 07:23:40.240016: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-06 07:23:40.959885: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/fuest/EnData/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "script_dir = Path().resolve()\n",
    "root_dir = (script_dir.parent)\n",
    "sys.path.append(str(root_dir))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datasets.pecanstreet import PecanStreetDataset\n",
    "from datasets.openpower import OpenPowerDataset\n",
    "from datasets.timeseries_dataset import TimeSeriesDataset\n",
    "from endata.trainer import Trainer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model from scratch ##\n",
    "\n",
    "To train your own model from scratch, the ` Trainer ` class provides a simple implementation. Simply define your a custom dataset and a ` Trainer ` object, and call the ` Trainer ` 's ` fit() ` method."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing a custom dataset ##\n",
    "\n",
    "When creating a custom time series dataset class for use with EnData, the class must inherit from the provided `TimeSeriesDataset` base class. The `TimeSeriesDataset` class provides a robust and modular framework for handling wide-format time series data. Custom implementations only need to define the `_preprocess_data` method, which is an abstract method in the base class. This method should ensure that the data is available in a clean wide-format data frame, that has the structure outlined below.\n",
    "\n",
    "### Responsibilities of `_preprocess_data`\n",
    "\n",
    "- Preprocess raw input data into a DataFrame that satisfies the expected structure.\n",
    "- Ensure time series columns contain arrays of the correct sequence length (`seq_len`).\n",
    "- Add any additional columns, such as entity identifiers or conditioning variables.\n",
    "\n",
    "### Benefits of the Base Class\n",
    "\n",
    "- **Normalization and Scaling:** Automatically handles standardization and min-max scaling.\n",
    "- **Conditioning Variables:** Provides support for encoding and managing conditioning variables.\n",
    "- **Time Series Merging and Splitting:** Facilitates operations to merge multiple time series columns into a single multidimensional array and split them back when needed.\n",
    "- **Data Transformation:** Includes functions for inverse transformations to revert normalized data to its original scale.\n",
    "\n",
    "---\n",
    "\n",
    "### Expected Input DataFrame Structure\n",
    "\n",
    "The input to the `TimeSeriesDataset` class must adhere to the following structure:\n",
    "\n",
    "| **Column Name**       | **Description**                                                                                     |\n",
    "|------------------------|-----------------------------------------------------------------------------------------------------|\n",
    "| `timeseries_col1`      | A column containing arrays of length `seq_len` (after preprocessing) representing the first dimension of the time series. |\n",
    "| `timeseries_col2`      | A column containing arrays of length `seq_len` (after preprocessing) representing the second dimension of the time series.|\n",
    "| `entity_column`        | A column containing unique identifiers for each entity (e.g., user, household, or device ID).       |\n",
    "| `conditioning_var1`    | An (optional) static or numeric conditioning variable (e.g., categorical or continuous feature).                |\n",
    "| `conditioning_var2`    | Further (optional) static or numeric conditioning variables.                                                    |\n",
    "\n",
    "- The `time_series_column_names` parameter specifies which columns are part of the time series.\n",
    "- The `entity_column_name` parameter identifies the column containing unique entity IDs.\n",
    "- The `conditioning_var_column_names` parameter defines additional conditioning variables.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTimeSeriesDataset(TimeSeriesDataset):\n",
    "    \"\"\"\n",
    "    A custom TimeSeriesDataset implementation for handling toy data.\n",
    "\n",
    "    Input data structure:\n",
    "    - time_series_col1, time_series_col2: Time series data with arrays of length seq_len.\n",
    "    - entity_id: Unique identifier for each entity.\n",
    "    - static_conditioning: Categorical or numeric conditioning variable.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        seq_len: int = 16,\n",
    "        normalize: bool = True,\n",
    "        scale: bool = True,\n",
    "    ):\n",
    "        entity_column_name = \"entity_id\"\n",
    "        time_series_column_names = [\"time_series_col1\", \"time_series_col2\"]\n",
    "        conditioning_var_column_names = [\"conditioning_var\"]\n",
    "\n",
    "        super().__init__(\n",
    "            data=data,\n",
    "            entity_column_name=entity_column_name,\n",
    "            time_series_column_names=time_series_column_names,\n",
    "            conditioning_var_column_names=conditioning_var_column_names,\n",
    "            seq_len=seq_len,\n",
    "            normalize=normalize,\n",
    "            scale=scale,\n",
    "        )\n",
    "\n",
    "    def _preprocess_data(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Preprocesses the raw input data to ensure it conforms to the expected format.\n",
    "\n",
    "        - Ensures time series columns contain arrays of length seq_len.\n",
    "        - Ensures all required columns are present.\n",
    "\n",
    "        Args:\n",
    "            data (pd.DataFrame): The raw input data.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The preprocessed data.\n",
    "        \"\"\"\n",
    "        required_columns = [\"entity_id\", \"time_series_col1\", \"time_series_col2\", \"conditioning_var\"]\n",
    "        for col in required_columns:\n",
    "            if col not in data.columns:\n",
    "                raise ValueError(f\"Missing required column: {col}\")\n",
    "\n",
    "        for col in [\"time_series_col1\", \"time_series_col2\"]:\n",
    "            data[col] = data[col].apply(\n",
    "                lambda x: np.array(x).reshape(-1, 1) if isinstance(x, list) else x\n",
    "            )\n",
    "            data[col] = data[col].apply(\n",
    "                lambda x: np.array(x) if isinstance(x, np.ndarray) else ValueError(f\"Invalid data in {col}\")\n",
    "            )\n",
    "        for col in [\"time_series_col1\", \"time_series_col2\"]:\n",
    "            data[col] = data[col].apply(\n",
    "                lambda x: x[:self.seq_len] if len(x) >= self.seq_len else ValueError(f\"Sequence too short in {col}\")\n",
    "            )\n",
    "        return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined our dataset class, let's create some artificial timeseries columns and conditioning variables which will comprise our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>entity_id</th>\n",
       "      <th>conditioning_var</th>\n",
       "      <th>timeseries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>entity_0</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.9260902409329174, 0.06733527088211624], [0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>entity_1</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.380586763778633, 0.9862005547191708], [0.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>entity_2</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.17112909695039222, 0.8879684137033145], [0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>entity_3</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.07133078623947496, 0.18731916651554545], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>entity_4</td>\n",
       "      <td>2</td>\n",
       "      <td>[[0.37708240761630424, 0.36632049150486373], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>entity_95</td>\n",
       "      <td>2</td>\n",
       "      <td>[[0.8441465706548078, 0.7879977379401687], [0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>entity_96</td>\n",
       "      <td>2</td>\n",
       "      <td>[[0.5815471394060646, 0.7404401475196428], [0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>entity_97</td>\n",
       "      <td>2</td>\n",
       "      <td>[[0.4270044026340649, 0.0861661115561432], [0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>entity_98</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.04971922238613394, 0.7569102697182223], [0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>entity_99</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.9316017709826142, 0.7785679215554204], [0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    index  entity_id  conditioning_var  \\\n",
       "0       0   entity_0                 0   \n",
       "1       1   entity_1                 0   \n",
       "2       2   entity_2                 1   \n",
       "3       3   entity_3                 1   \n",
       "4       4   entity_4                 2   \n",
       "..    ...        ...               ...   \n",
       "95     95  entity_95                 2   \n",
       "96     96  entity_96                 2   \n",
       "97     97  entity_97                 2   \n",
       "98     98  entity_98                 1   \n",
       "99     99  entity_99                 0   \n",
       "\n",
       "                                           timeseries  \n",
       "0   [[0.9260902409329174, 0.06733527088211624], [0...  \n",
       "1   [[0.380586763778633, 0.9862005547191708], [0.4...  \n",
       "2   [[0.17112909695039222, 0.8879684137033145], [0...  \n",
       "3   [[0.07133078623947496, 0.18731916651554545], [...  \n",
       "4   [[0.37708240761630424, 0.36632049150486373], [...  \n",
       "..                                                ...  \n",
       "95  [[0.8441465706548078, 0.7879977379401687], [0....  \n",
       "96  [[0.5815471394060646, 0.7404401475196428], [0....  \n",
       "97  [[0.4270044026340649, 0.0861661115561432], [0....  \n",
       "98  [[0.04971922238613394, 0.7569102697182223], [0...  \n",
       "99  [[0.9316017709826142, 0.7785679215554204], [0....  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame({\n",
    "        \"entity_id\": [f\"entity_{i}\" for i in range(100)],\n",
    "        \"time_series_col1\": [np.random.rand(16).tolist() for _ in range(100)],\n",
    "        \"time_series_col2\": [np.random.rand(16).tolist() for _ in range(100)],\n",
    "        \"conditioning_var\": np.random.choice([\"a\", \"b\", \"c\"], size=100).tolist(),\n",
    "    })\n",
    "\n",
    "custom_dataset = CustomTimeSeriesDataset(data)\n",
    "custom_dataset.data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create a `Trainer` object by passing the name of the desired model and the dataset object. To start training, simply call `Trainer.fit()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fuest/EnData/venv/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Training: 100%|██████████| 1000/1000 [00:57<00:00, 17.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model_name=\"diffusion_ts\", dataset=custom_dataset)\n",
    "trainer.fit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once training is complete, we can create a data generator object that has access to the trained model and dataset information. To generate data, there is no need to load in a trained model. Simply define the conditioning variables, and call the `DataGenerator` 's `generate()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conditioning_var': {0: 'a', 1: 'b', 2: 'c'}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_generator = trainer.get_data_generator()\n",
    "conditioning_var_codes = data_generator.get_conditioning_var_codes()\n",
    "conditioning_var_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 1000/1000 [00:16<00:00, 62.15it/s]\n"
     ]
    }
   ],
   "source": [
    "conditioning_vars = {\n",
    "    \"conditioning_var\": 0\n",
    "}\n",
    "data_generator.set_model_conditioning_vars(conditioning_vars)\n",
    "generated_df = data_generator.generate(num_samples=100)\n",
    "generated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5ea03d6e3b6d6622a3c4a21d472827b30621b9677ee232f2748a7c51f6a29b0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
