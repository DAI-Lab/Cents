device: 1 # 0, cpu
seq_len: 96 # should not be changed for the current datasets
input_dim: 2 # or 1 depending on user, but is dynamically set
noise_dim: 256
cond_emb_dim: 64
shuffle: True
sparse_conditioning_loss_weight: 0.8 # sparse conditioning training sample weight for loss computation [0, 1]
freeze_cond_after_warmup: False # specify whether to freeze conditioning module parameters after warmup epochs

conditioning_vars: # for each desired conditioning variable, add the name and number of categories
  month: 12
  weekday: 7
  building_type: 3
  has_solar: 2 # note that the metadata column name is 'solar', which is renamed to avoid conflicts with the 'solar' column in the other csv.
  car1: 2
  city: 7
  state: 3
  total_square_footage: 5
  house_construction_year: 5

diffcharge:
  batch_size: 64
  n_epochs: 1000
  init_lr: 3e-5
  network: cnn # attention
  guidance_scale: 1.2
  hidden_dim: 256
  nhead: 8
  beta_start: 1e-4
  beta_end: 0.02
  n_steps: 1000
  schedule: linear # quadratic, cosine
  warm_up_epochs: 100

diffusion_ts:
  batch_size: 64
  n_epochs: 1000
  n_steps: 1000
  base_lr: 1e-4
  n_layer_enc: 4
  n_layer_dec: 5
  d_model: 256
  sampling_timesteps: null
  loss_type: l1 #l2
  beta_schedule: cosine #linear
  n_heads: 4
  mlp_hidden_times: 4
  eta: 0.0
  attn_pd: 0.0
  resid_pd: 0.0
  kernel_size: null
  padding_size: null
  use_ff: true
  reg_weight: null
  results_folder: ./Checkpoints_syn
  gradient_accumulate_every: 2
  save_cycle: 1000
  ema_decay: 0.99
  ema_update_interval: 10
  lr_scheduler_params:
    factor: 0.5
    patience: 200
    min_lr: 1.0e-5
    threshold: 1.0e-1
    threshold_mode: rel
    verbose: false
  warm_up_epochs: 100

acgan:
  batch_size: 32
  n_epochs: 200
  lr_gen: 3e-4
  lr_discr: 1e-4
  warm_up_epochs: 100
  include_auxiliary_losses: True
