{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ny_path = \"/home/fuest/EnData/data/pecanstreet/15minute_data_newyork.csv\"\n",
    "austin_path = \"/home/fuest/EnData/data/pecanstreet/15minute_data_austin.csv\"\n",
    "cali_path = \"/home/fuest/EnData/data/pecanstreet/15minute_data_california.csv\"\n",
    "\n",
    "ny_data = pd.read_csv(ny_path)\n",
    "austin_data = pd.read_csv(austin_path)\n",
    "cali_data = pd.read_csv(cali_path)\n",
    "\n",
    "ny_user_ids = ny_data.dataid.unique().tolist()\n",
    "austin_user_ids = austin_data.dataid.unique().tolist()\n",
    "cali_user_ids = cali_data.dataid.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3687,\n",
       " 6377,\n",
       " 7062,\n",
       " 8574,\n",
       " 9213,\n",
       " 203,\n",
       " 1450,\n",
       " 1524,\n",
       " 2606,\n",
       " 3864,\n",
       " 7114,\n",
       " 1731,\n",
       " 4495,\n",
       " 8342,\n",
       " 3938,\n",
       " 5938,\n",
       " 8061,\n",
       " 9775,\n",
       " 4934,\n",
       " 8733,\n",
       " 9612,\n",
       " 9836,\n",
       " 6547,\n",
       " 661,\n",
       " 1642,\n",
       " 2335,\n",
       " 2361,\n",
       " 2818,\n",
       " 3039,\n",
       " 3456,\n",
       " 3538,\n",
       " 4031,\n",
       " 4373,\n",
       " 4767,\n",
       " 5746,\n",
       " 6139,\n",
       " 7536,\n",
       " 7719,\n",
       " 7800,\n",
       " 7901,\n",
       " 7951,\n",
       " 8156,\n",
       " 8386,\n",
       " 8565,\n",
       " 9019,\n",
       " 9160,\n",
       " 9922,\n",
       " 9278,\n",
       " 4550,\n",
       " 558,\n",
       " 2358,\n",
       " 3700,\n",
       " 1417,\n",
       " 5679,\n",
       " 5058,\n",
       " 2318,\n",
       " 5997,\n",
       " 950,\n",
       " 5982,\n",
       " 5587,\n",
       " 1222,\n",
       " 387,\n",
       " 3000,\n",
       " 4283,\n",
       " 3488,\n",
       " 3517,\n",
       " 9053,\n",
       " 3996,\n",
       " 27,\n",
       " 142,\n",
       " 914,\n",
       " 2096,\n",
       " 1240]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cali_user_ids + austin_user_ids + ny_user_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_range_with_syn_values(df, colname, syn_df, month, weekday):\n",
    "    # Filter the DataFrame for the given month and weekday\n",
    "    filtered_df = df[(df['month'] == month) & (df['weekday'] == weekday)]\n",
    "    \n",
    "    # Ensure there is data for the given month and weekday\n",
    "    if filtered_df.empty:\n",
    "        print(f\"No data for month={month}, weekday={weekday}\")\n",
    "        return\n",
    "    \n",
    "    # Convert the 'grid' column to a numpy array\n",
    "    array_data = np.array(filtered_df[colname].to_list())\n",
    "\n",
    "    # Calculate min and max values across all rows\n",
    "    min_values = np.min(array_data, axis=0)\n",
    "    max_values = np.max(array_data, axis=0)\n",
    "\n",
    "    # Ensure the synthetic data DataFrame has the same structure and filter it\n",
    "    syn_filtered_df = syn_df[(syn_df['month'] == month) & (syn_df['weekday'] == weekday)]\n",
    "\n",
    "    # Ensure there is synthetic data for the given month and weekday\n",
    "    if syn_filtered_df.empty:\n",
    "        print(f\"No synthetic data for month={month}, weekday={weekday}\")\n",
    "        return\n",
    "\n",
    "    syn_values = np.array(syn_filtered_df['grid'].to_list()).squeeze()\n",
    "\n",
    "    # Generate timestamps for the x-axis\n",
    "    timestamps = pd.date_range(start='00:00', end='23:45', freq='15T').strftime('%H:%M')\n",
    "\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    plt.fill_between(timestamps, min_values, max_values, color='gray', alpha=0.5, label='Range of values')\n",
    "    \n",
    "    # Plot the synthetic time series\n",
    "    plt.plot(timestamps, syn_values[2], color='blue', marker='o', markersize=2, linestyle='-', label='Synthetic values')\n",
    "\n",
    "    plt.title(f'Range of Values and Synthetic Data Comparison for Month={month}, Weekday={weekday}')\n",
    "    plt.xlabel('Time of Day')\n",
    "    plt.ylabel('Values')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_range_with_syn_values(unnormalized_ori, 'grid', unnormalized_syn, 10, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from data_utils.dataset import PecanStreetDataset, split_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def check_inverse_transform(normalized_dataset, unnormalized_dataset):\n",
    "    normalized_df = normalized_dataset.data\n",
    "    unnormalized_df = unnormalized_dataset.data\n",
    "    transformed = normalized_dataset.inverse_transform(normalized_df)\n",
    "\n",
    "    mse_list = []\n",
    "\n",
    "    for idx in range(len(unnormalized_df)):\n",
    "        unnormalized_timeseries = unnormalized_df.iloc[idx][\"timeseries\"]\n",
    "        transformed_timeseries = transformed.iloc[idx][\"timeseries\"]\n",
    "\n",
    "        # Ensure the arrays have the same shape\n",
    "        assert unnormalized_timeseries.shape == transformed_timeseries.shape, \"Shape mismatch between transformed and unnormalized timeseries\"\n",
    "\n",
    "        # Calculate the MSE for this row\n",
    "        mse = mean_squared_error(unnormalized_timeseries, transformed_timeseries)\n",
    "        mse_list.append(mse)\n",
    "\n",
    "    # Calculate the average MSE over all rows\n",
    "    avg_mse = np.mean(mse_list)\n",
    "\n",
    "    print(f\"Average MSE over all rows: {avg_mse}\")\n",
    "    return avg_mse\n",
    "\n",
    "check_inverse_transform(661)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "USER_IDs = [27,  142,  203,  387,  558,  661,  914,  950, 1222, 1240, 1417,\n",
    "       1450, 1524, 1642, 1731, 2096, 2318, 2335, 2358, 2361, 2606, 2818,\n",
    "       3000, 3039, 3456, 3488, 3517, 3538, 3687, 3700, 3864, 3938, 3996,\n",
    "       4031, 4283, 4373, 4495, 4550, 4767, 4934, 5058, 5587, 5679, 5746,\n",
    "       5938, 5982, 5997, 6139, 6377, 6547, 7062, 7114, 7536, 7719, 7800,\n",
    "       7901, 7951, 8061, 8156, 8342, 8386, 8565, 8574, 8733, 9019, 9053,\n",
    "       9160, 9213, 9278, 9612, 9775, 9836, 9922]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fuest/EnData/venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.32s/it]\n"
     ]
    }
   ],
   "source": [
    "from generator.llm.llm import HF\n",
    "from generator.llm.preprocessing import Signal2String\n",
    "\n",
    "model = HF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils.dataset import PecanStreetDataset\n",
    "\n",
    "data = PecanStreetDataset(normalize=True, include_generation=False)\n",
    "user_data = data.create_user_dataset(661)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = Signal2String(decimal=4)\n",
    "text = converter.transform(user_data.data.timeseries.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.replace('[', '').replace(\"]\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The model vocabulary size is 128256, but the following tokens were being biased: [128256]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mgenerate(text)\n",
      "File \u001b[0;32m~/EnData/generator/llm/llm.py:229\u001b[0m, in \u001b[0;36mHF.generate\u001b[0;34m(self, text, length, temp, top_p, raw, samples, padding)\u001b[0m\n\u001b[1;32m    226\u001b[0m average_length \u001b[39m=\u001b[39m input_length \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(text\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m    227\u001b[0m max_tokens \u001b[39m=\u001b[39m (average_length \u001b[39m+\u001b[39m padding) \u001b[39m*\u001b[39m length\n\u001b[0;32m--> 229\u001b[0m generate_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m    230\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtokenized_input,\n\u001b[1;32m    231\u001b[0m     do_sample\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    232\u001b[0m     max_new_tokens\u001b[39m=\u001b[39;49mmax_tokens,\n\u001b[1;32m    233\u001b[0m     temperature\u001b[39m=\u001b[39;49mtemp,\n\u001b[1;32m    234\u001b[0m     top_p\u001b[39m=\u001b[39;49mtop_p,\n\u001b[1;32m    235\u001b[0m     bad_words_ids\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minvalid_tokens,\n\u001b[1;32m    236\u001b[0m     renormalize_logits\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    237\u001b[0m     num_return_sequences\u001b[39m=\u001b[39;49msamples\n\u001b[1;32m    238\u001b[0m )\n\u001b[1;32m    240\u001b[0m responses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mbatch_decode(\n\u001b[1;32m    241\u001b[0m     generate_ids[:, input_length:],\n\u001b[1;32m    242\u001b[0m     skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    243\u001b[0m     clean_up_tokenization_spaces\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    244\u001b[0m )\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m raw:\n",
      "File \u001b[0;32m~/EnData/venv/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/EnData/venv/lib/python3.8/site-packages/transformers/generation/utils.py:2024\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2016\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2017\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   2018\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2019\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2020\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2021\u001b[0m     )\n\u001b[1;32m   2023\u001b[0m     \u001b[39m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2024\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sample(\n\u001b[1;32m   2025\u001b[0m         input_ids,\n\u001b[1;32m   2026\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mprepared_logits_processor,\n\u001b[1;32m   2027\u001b[0m         logits_warper\u001b[39m=\u001b[39;49mprepared_logits_warper,\n\u001b[1;32m   2028\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mprepared_stopping_criteria,\n\u001b[1;32m   2029\u001b[0m         generation_config\u001b[39m=\u001b[39;49mgeneration_config,\n\u001b[1;32m   2030\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   2031\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   2032\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   2033\u001b[0m     )\n\u001b[1;32m   2035\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39min\u001b[39;00m (GenerationMode\u001b[39m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[39m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2036\u001b[0m     \u001b[39m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   2037\u001b[0m     prepared_logits_warper \u001b[39m=\u001b[39m (\n\u001b[1;32m   2038\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(generation_config, device\u001b[39m=\u001b[39minput_ids\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2039\u001b[0m         \u001b[39mif\u001b[39;00m generation_config\u001b[39m.\u001b[39mdo_sample\n\u001b[1;32m   2040\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2041\u001b[0m     )\n",
      "File \u001b[0;32m~/EnData/venv/lib/python3.8/site-packages/transformers/generation/utils.py:2992\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2989\u001b[0m next_token_logits \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlogits[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\u001b[39m.\u001b[39mclone()\n\u001b[1;32m   2991\u001b[0m \u001b[39m# pre-process distribution\u001b[39;00m\n\u001b[0;32m-> 2992\u001b[0m next_token_scores \u001b[39m=\u001b[39m logits_processor(input_ids, next_token_logits)\n\u001b[1;32m   2993\u001b[0m \u001b[39mif\u001b[39;00m do_sample:\n\u001b[1;32m   2994\u001b[0m     next_token_scores \u001b[39m=\u001b[39m logits_warper(input_ids, next_token_scores)\n",
      "File \u001b[0;32m~/EnData/venv/lib/python3.8/site-packages/transformers/generation/logits_process.py:98\u001b[0m, in \u001b[0;36mLogitsProcessorList.__call__\u001b[0;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m         scores \u001b[39m=\u001b[39m processor(input_ids, scores, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     97\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 98\u001b[0m         scores \u001b[39m=\u001b[39m processor(input_ids, scores)\n\u001b[1;32m    100\u001b[0m \u001b[39mreturn\u001b[39;00m scores\n",
      "File \u001b[0;32m~/EnData/venv/lib/python3.8/site-packages/transformers/generation/logits_process.py:1119\u001b[0m, in \u001b[0;36mSequenceBiasLogitsProcessor.__call__\u001b[0;34m(self, input_ids, scores)\u001b[0m\n\u001b[1;32m   1115\u001b[0m \u001b[39m@add_start_docstrings\u001b[39m(LOGITS_PROCESSOR_INPUTS_DOCSTRING)\n\u001b[1;32m   1116\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, input_ids: torch\u001b[39m.\u001b[39mLongTensor, scores: torch\u001b[39m.\u001b[39mFloatTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mFloatTensor:\n\u001b[1;32m   1117\u001b[0m     \u001b[39m# 1 - Prepares the bias tensors. This is only needed the first time the logit processor is called.\u001b[39;00m\n\u001b[1;32m   1118\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepared_bias_variables:\n\u001b[0;32m-> 1119\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_bias_variables(scores)\n\u001b[1;32m   1121\u001b[0m     \u001b[39m# 2 - prepares an empty bias to add\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m     bias \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros_like(scores)\n",
      "File \u001b[0;32m~/EnData/venv/lib/python3.8/site-packages/transformers/generation/logits_process.py:1159\u001b[0m, in \u001b[0;36mSequenceBiasLogitsProcessor._prepare_bias_variables\u001b[0;34m(self, scores)\u001b[0m\n\u001b[1;32m   1157\u001b[0m             invalid_biases\u001b[39m.\u001b[39mappend(token_id)\n\u001b[1;32m   1158\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(invalid_biases) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 1159\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1160\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe model vocabulary size is \u001b[39m\u001b[39m{\u001b[39;00mvocabulary_size\u001b[39m}\u001b[39;00m\u001b[39m, but the following tokens were being biased: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1161\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00minvalid_biases\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1162\u001b[0m     )\n\u001b[1;32m   1164\u001b[0m \u001b[39m# Precompute the bias tensors to be applied. Sequences of length 1 are kept separately, as they can be applied\u001b[39;00m\n\u001b[1;32m   1165\u001b[0m \u001b[39m# with simpler logic.\u001b[39;00m\n\u001b[1;32m   1166\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength_1_bias \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros((vocabulary_size,), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat)\u001b[39m.\u001b[39mto(scores\u001b[39m.\u001b[39mdevice)\n",
      "\u001b[0;31mValueError\u001b[0m: The model vocabulary size is 128256, but the following tokens were being biased: [128256]"
     ]
    }
   ],
   "source": [
    "model.generate(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9396,9999,8915,6912,6774,6214,5821,6016,5857,5673,5714,5891,5714,6218,7777,5936,5334,5301,5332,5341,4924,4887,4853,4935,4809,4770,4760,4932,4886,4781,4755,4819,4590,4712,4673,4711,4604,4766,4666,4837,4596,4780,4588,4844,4714,4728,4676,4736,4637,4896,4655,4803,4645,4868,4783,4781,3938,3597,3506,2740,2289,1785,1763,1617,1189,907,827,808,599,426,301,408,308,357,215,337,257,376,473,586,580,715,1000,1270,1265,1528,1971,2626,2768,3007,3340,3850,4041,4266,4841,5139'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Jul 29 2024, 17:02:10) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5ea03d6e3b6d6622a3c4a21d472827b30621b9677ee232f2748a7c51f6a29b0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
